
# Censorship Auto-Collection Toolkit

This repo helps you **automatically collect government-level platform censorship cases** and convert them into a **clean, analysis-ready dataset**.

---

## Folder Structure (Final)

```
censorship_autocollect/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ case_schema.csv                # REQUIRED: master dataset columns
â”‚   â”œâ”€â”€ features_reference.csv         # REQUIRED: platform feature taxonomy
â”‚   â””â”€â”€ platform_profile.csv           # OPTIONAL: user-provided metadata
â”‚
â”œâ”€â”€ output/                            # directory to save outputs
â”‚   â”œâ”€â”€ ooni_results_*.csv
â”‚   â”œâ”€â”€ stop_platform_cases.csv
â”‚   â”œâ”€â”€ netblocks_platform_cases.json
â”‚   â”œâ”€â”€ cases_llm.jsonl
â”‚   â”œâ”€â”€ case_schema_annotated.csv
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ooni_fetch.py                  # REQUIRED: robust OONI API fetcher
â”‚   â”œâ”€â”€ accessnow_stop_fetch.py        # REQUIRED: AccessNow STOP filter
â”‚   â”œâ”€â”€ netblocks_scrape.py            # REQUIRED: NetBlocks report scraper
â”‚   â”œâ”€â”€ jsonl_to_csv.py                # Append LLM JSON â†’ case_schema.csv
â”‚   â””â”€â”€ annotate_cases_with_features.py# Join platform metadata
â”‚
â”œâ”€â”€ llm_prompts/
â”‚   â””â”€â”€ case_label_prompt.txt          # REQUIRED: convert incident â†’ JSON
â”‚
â””â”€â”€ README.md
```

---

## Quickstart

```
python -V   # Python 3.9â€“3.12
pip install requests pandas beautifulsoup4 python-dateutil
```

---

# Workflow Overview

1) **OONI measurements â†’** `output/ooni_results.csv`  
2) **STOP shutdowns â†’** `output/stop_platform_cases.csv`  
3) **NetBlocks reports â†’** `output/netblocks_platform_cases.json`  
4) **LLM conversion (prompt)**: â†’ `output/cases_llm.jsonl`  
5) **Append to master**: JSONL â†’ `data/case_schema.csv`  
6) **(Optional) Feature annotation**: join `platform_profile.csv` â†’ `output/case_schema_annotated.csv`  
7) **Analyze** (notebook/script)

---

# Step-by-Step

## Step 1 â€” OONI Measurements

```
python scripts/ooni_fetch_windowed.py \
  --domains tiktok.com,twitter.com,facebook.com,instagram.com,youtube.com,whatsapp.com,telegram.org,reddit.com,snapchat.com,linkedin.com,wechat.com,vk.com,signal.org,pinterest.com,discord.com,tumblr.com,line.me,medium.com,viber.com,threads.net \
  --countries IN,TR,RU,IR,SA,AE,EG,IQ,LB,ET,UG,SD,NG,KE,CN,PK,MM,TH,VN,SY,CU \
  --since 2020-01-01 \
  --until 2025-10-30 \
  --limit 200 --timeout 180 --retries 5 --sleep 1.5 \
  --confirmed_only \
  --out output/ooni_results_blocked_window.csv
```

### Useful Flags
| Flag | Meaning |
|------|--------|
| `--confirmed_only` | Only confirmed blocks |
| `--limit` | Per request page size |
| `--timeout` | Request timeout |
| `--retries` | Retry attempts |
| `--sleep` | Delay per page |

ðŸ’¡ Start with few domains/countries â†’ scale up

---

## Step 2 â€” AccessNow STOP Filter

```
python scripts/accessnow_stop_fetch.py \
  --csv /path/to/STOP.csv \
  --out output/stop_platform_cases.csv \
  --keywords "Twitter,Facebook,TikTok,YouTube,Telegram,Instagram,WhatsApp,Signal,Snapchat,Reddit"
```

---

## Step 3 â€” NetBlocks Reports

```
python scripts/netblocks_scrape.py \
  --pages 5 \
  --out output/netblocks_platform_cases.json
```

---

## Step 4 â€” LLM Case Conversion â†’ JSONL

Convert incident text â†’ structured JSON with:
```
llm_prompts/case_label_prompt.txt
```

Each JSON record â†’ **one line**:
```
output/cases_llm.jsonl
```

Example:
```json
{"case_id":"IN-20200629-TIKTOK","country":"India","platform":"TikTok","start_date":"2020-06-29"}
```

---

## Step 5 â€” Append JSON â†’ Master CSV

```
python scripts/jsonl_to_csv.py \
  --in_jsonl output/cases_llm.jsonl \
  --out_csv data/case_schema.csv \
  --touch_last_updated
```

---

## Step 6 â€” Optional: Feature Annotation

Create once:

```
data/platform_profile.csv
```

Then run:
```
python scripts/annotate_cases_with_features.py \
  --cases_csv data/case_schema.csv \
  --profile_csv data/platform_profile.csv \
  --out_csv output/case_schema_annotated.csv
```

---

# Tips & Troubleshooting

- If OONI runs fast â†’ likely low matches  
- Avoid timeouts â†’ reduce date window, limit, split countries  
- Save raw pull results under `output/`  
- Ensure JSON keys match `case_schema.csv`

---

# Ethics & Notes
- Respect robots.txt and API rate limits
- Keep original source URLs
- Document LLM + settings

---

Happy researching!
